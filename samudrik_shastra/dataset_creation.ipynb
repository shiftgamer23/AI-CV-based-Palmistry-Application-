{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "763375c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SID\\OneDrive\\Desktop\\AIML Projs\\Palmistry CV\\palmistry_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\SID\\OneDrive\\Desktop\\AIML Projs\\Palmistry CV\\palmistry_env\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 - imports & config\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "# local config\n",
    "SAMUDRIK_TXT = \"hasta_samudrik_cleaned.txt\"   # path to your book text\n",
    "OUTPUT_JSONL = \"json_to_interpretation_pairs.jsonl\"\n",
    "INDEX_DIR = \"faiss_index\"\n",
    "CHUNK_SIZE = 800            # characters per chunk\n",
    "CHUNK_OVERLAP = 200\n",
    "EMB_MODEL = \"all-MiniLM-L6-v2\"   # small & fast\n",
    "EMB_DIM = 384\n",
    "TOP_K = 5\n",
    "NUM_PAIRS = 1000            # target number of pairs to produce\n",
    "OLLAMA_URL = os.getenv(\"OLLAMA_URL\", \"http://localhost:11434\")\n",
    "OLLAMA_MODEL = os.getenv(\"OLLAMA_MODEL\", \"mistral:7b-instruct-q4_0\")  # set to the model you pulled\n",
    "BATCH_JSONS_PER_CALL = 10   # how many random JSONs to request per Ollama call\n",
    "OLLAMA_SLEEP = 0.08         # small delay between Ollama requests\n",
    "RAG_TEMPERATURE = 0.2       # low temp for grounded interpretations\n",
    "RNG_SEED = 42\n",
    "\n",
    "# numeric ranges that match typical CV outputs; tune as needed\n",
    "BREAKS_RANGE = (0, 10)\n",
    "BRANCHES_RANGE = (0, 300)\n",
    "\n",
    "random.seed(RNG_SEED)\n",
    "np.random.seed(RNG_SEED)\n",
    "\n",
    "Path(INDEX_DIR).mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2a98634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 750 chunks.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from typing import List\n",
    "\n",
    "CHUNK_SIZE = 800  # adjust if needed\n",
    "CHUNK_OVERLAP = 100\n",
    "\n",
    "def chunk_file(path: str, chunk_size: int = CHUNK_SIZE, overlap: int = CHUNK_OVERLAP) -> List[str]:\n",
    "    chunks = []\n",
    "    buffer = \"\"\n",
    "\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            buffer += line.strip() + \" \"\n",
    "            while len(buffer) > chunk_size:\n",
    "                # cut at chunk_size\n",
    "                j = chunk_size\n",
    "                # try to cut nicely at sentence boundary\n",
    "                tail = buffer[j:j+200]\n",
    "                m = re.search(r'([.?!]\\s)', tail)\n",
    "                if m:\n",
    "                    j = j + m.end()\n",
    "                chunk = buffer[:j].strip()\n",
    "                chunks.append(chunk)\n",
    "                # prepare buffer for next chunk (with overlap)\n",
    "                buffer = buffer[j - overlap:]\n",
    "\n",
    "    # add any leftover\n",
    "    if buffer.strip():\n",
    "        chunks.append(buffer.strip())\n",
    "\n",
    "    return chunks\n",
    "\n",
    "chunks = chunk_file(SAMUDRIK_TXT)\n",
    "print(f\"Created {len(chunks)} chunks.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a693e526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing FAISS index & metadata ...\n"
     ]
    }
   ],
   "source": [
    "# Cell 3 - embeddings + FAISS index\n",
    "embedder = SentenceTransformer(EMB_MODEL)\n",
    "\n",
    "INDEX_PATH = Path(INDEX_DIR) / \"index.faiss\"\n",
    "META_PATH = Path(INDEX_DIR) / \"meta.json\"\n",
    "\n",
    "def build_or_load_index(chunks: List[str]):\n",
    "    if INDEX_PATH.exists() and META_PATH.exists():\n",
    "        print(\"Loading existing FAISS index & metadata ...\")\n",
    "        index = faiss.read_index(str(INDEX_PATH))\n",
    "        with open(META_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "            metas = json.load(f)\n",
    "        return index, metas\n",
    "    print(\"Computing embeddings and building FAISS index (this may take a moment)...\")\n",
    "    embeddings = []\n",
    "    batch = 64\n",
    "    for i in range(0, len(chunks), batch):\n",
    "        batch_texts = chunks[i:i+batch]\n",
    "        emb = embedder.encode(batch_texts, show_progress_bar=False, convert_to_numpy=True)\n",
    "        embeddings.append(emb)\n",
    "    embeddings = np.vstack(embeddings).astype('float32')\n",
    "    # normalize for IP search\n",
    "    faiss.normalize_L2(embeddings)\n",
    "    index = faiss.IndexFlatIP(EMB_DIM)\n",
    "    index.add(embeddings)\n",
    "    faiss.write_index(index, str(INDEX_PATH))\n",
    "    metas = [{\"id\": idx, \"text\": chunks[idx]} for idx in range(len(chunks))]\n",
    "    with open(META_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metas, f, ensure_ascii=False, indent=2)\n",
    "    print(\"Index built and saved.\")\n",
    "    return index, metas\n",
    "\n",
    "index, metas = build_or_load_index(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6bd7672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample retrieval (top2):\n",
      "208 0.594800591468811 , in many cases, death, unless this break is supported bv a sister line or is enclosed in a square. But the break should be verified from both the hands. If the line is laddered and broken up, a perio...\n",
      "619 0.55412757396698 him with the gifts of organization and administration denoted, by a straight line of Head. Fic. 60. THE HAND OF A POLITICIAN AND ADMINISTRATOR 1. Double line of Life. 2. More than three up-going lines...\n"
     ]
    }
   ],
   "source": [
    "# Cell 4 - retrieval helper\n",
    "def retrieve_passages_for_json(json_input: Dict, top_k: int = TOP_K) -> List[Tuple[int, float, str]]:\n",
    "    # build simple text query summarizing JSON features\n",
    "    parts = []\n",
    "    for ln, info in json_input[\"lines\"].items():\n",
    "        if not info.get(\"present\"):\n",
    "            parts.append(f\"{ln}:absent\")\n",
    "        else:\n",
    "            parts.append(f\"{ln}:{info.get('length','')},{info.get('clarity','')},breaks={info.get('breaks',0)},branches={info.get('branches',0)}\")\n",
    "    query = \" ; \".join(parts)\n",
    "    q_emb = embedder.encode([query], convert_to_numpy=True).astype('float32')\n",
    "    faiss.normalize_L2(q_emb)\n",
    "    D, I = index.search(q_emb, top_k)\n",
    "    idxs, scores = I[0], D[0]\n",
    "    results = []\n",
    "    for idx, score in zip(idxs, scores):\n",
    "        if idx < 0:\n",
    "            continue\n",
    "        results.append((int(idx), float(score), metas[idx][\"text\"]))\n",
    "    return results\n",
    "\n",
    "# quick test retrieval\n",
    "test_json = {\"lines\": {\"life_line\": {\"present\": True, \"clarity\": \"clear\", \"length\": \"long\", \"breaks\": 1, \"branches\": 5},\n",
    "                       \"head_line\": {\"present\": False, \"clarity\": \"\", \"length\": \"\", \"breaks\": 0, \"branches\": 0},\n",
    "                       \"heart_line\": {\"present\": False, \"clarity\": \"\", \"length\": \"\", \"breaks\": 0, \"branches\": 0},\n",
    "                       \"fate_line\": {\"present\": False, \"clarity\": \"\", \"length\": \"\", \"breaks\": 0, \"branches\": 0}}}\n",
    "print(\"Sample retrieval (top2):\")\n",
    "for pid, sc, txt in retrieve_passages_for_json(test_json, top_k=2):\n",
    "    print(pid, sc, txt[:200].replace(\"\\n\",\" \") + \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "caf5ade6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 - Ollama call and parse utilities\n",
    "OLLAMA_GENERATE_ENDPOINT = OLLAMA_URL.rstrip(\"/\") + \"/api/generate\"\n",
    "JSON_ARRAY_RE = re.compile(r'\\[.*\\]', re.S)\n",
    "JSON_OBJ_RE = re.compile(r'\\{.*\\}', re.S)\n",
    "\n",
    "def call_ollama(prompt: str, model: str = OLLAMA_MODEL, max_tokens: int = 1024, temperature: float = 1.0, timeout: int = 60) -> str:\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"stream\": False   # 👈 IMPORTANT\n",
    "    }\n",
    "    r = requests.post(OLLAMA_GENERATE_ENDPOINT, json=payload, timeout=timeout)\n",
    "    r.raise_for_status()\n",
    "    dd = r.json()\n",
    "    \n",
    "    if isinstance(dd, dict) and \"response\" in dd:\n",
    "        return dd[\"response\"]\n",
    "    return str(dd)\n",
    "\n",
    "\n",
    "def extract_json_list_from_text(text: str) -> List[dict]:\n",
    "    \"\"\"Try to extract a JSON array from text; if only object found, wrap it into a list.\"\"\"\n",
    "    text = text.strip()\n",
    "    m = JSON_ARRAY_RE.search(text)\n",
    "    if m:\n",
    "        s = m.group(0)\n",
    "    else:\n",
    "        m2 = JSON_OBJ_RE.search(text)\n",
    "        s = m2.group(0) if m2 else text\n",
    "        if s and not s.strip().startswith(\"[\"):\n",
    "            s = \"[\" + s + \"]\"\n",
    "    try:\n",
    "        parsed = json.loads(s)\n",
    "        if isinstance(parsed, dict):\n",
    "            return [parsed]\n",
    "        return parsed\n",
    "    except Exception as e:\n",
    "        # attempt simple fixes: single quotes -> double quotes\n",
    "        try:\n",
    "            fixed = s.replace(\"'\", '\"')\n",
    "            parsed = json.loads(fixed)\n",
    "            return parsed if isinstance(parsed, list) else [parsed]\n",
    "        except Exception:\n",
    "            raise RuntimeError(f\"Failed to parse JSON from Ollama output. Raw text:\\n{s[:1000]}\\nError: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7ed30f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6 - prompt builders\n",
    "\n",
    "def prompt_generate_jsons(batch_n: int = BATCH_JSONS_PER_CALL) -> str:\n",
    "    # ask for an array of JSON objects to speed up generation\n",
    "    return f\"\"\"\n",
    "Generate exactly {batch_n} RANDOM JSON objects, and return them as a single JSON ARRAY (i.e. start with '[' and end with ']').\n",
    "Each object must follow this EXACT schema (use only these fields and types):\n",
    "\n",
    "{{ \"lines\": {{\n",
    "  \"life_line\":  {{ \"present\": true|false, \"clarity\": \"clear|faint|broken\", \"length\": \"short|medium|long\", \"breaks\": <int>, \"branches\": <int> }},\n",
    "  \"head_line\":  {{ \"present\": true|false, \"clarity\": \"clear|faint|broken\", \"length\": \"short|medium|long\", \"breaks\": <int>, \"branches\": <int> }},\n",
    "  \"heart_line\": {{ \"present\": true|false, \"clarity\": \"clear|faint|broken\", \"length\": \"short|medium|long\", \"breaks\": <int>, \"branches\": <int> }},\n",
    "  \"fate_line\":  {{ \"present\": true|false, \"clarity\": \"clear|faint|broken\", \"length\": \"short|medium|long\", \"breaks\": <int>, \"branches\": <int> }}\n",
    "}} }}\n",
    "\n",
    "Rules:\n",
    "- If 'present' is false for a line, then 'clarity' and 'length' MUST be empty strings (\"\"), and 'breaks' & 'branches' MUST be 0.\n",
    "- If 'present' is true, pick clarity from (clear,faint,broken), length from (short,medium,long).\n",
    "- Use realistic integers: breaks (0-12), branches (0-500).\n",
    "- Return ONLY the JSON ARRAY and nothing else (no commentary, no numbering).\n",
    "- Produce varied combinations (not the same repeated object).\n",
    "\"\"\"\n",
    "def prompt_rag_interpretation(json_input: dict, passages: List[Tuple[int,float,str]]) -> str:\n",
    "    # Build prompt that includes retrieved passages and strict instruction to use only them\n",
    "    header = (\"You are an expert in traditional Indian palmistry (Hasta Samudrik Shastra). \"\n",
    "              \"Using ONLY the passages provided below (do not invent or add extra classical references), \"\n",
    "              \"write a concise interpretation (2-6 sentences) for the following palm features in JSON form. \"\n",
    "              \"If no passage directly supports a certain feature, do not assert it.\\n\\n\")\n",
    "    json_block = \"Palm features (JSON):\\n\" + json.dumps(json_input, ensure_ascii=False, indent=2) + \"\\n\\n\"\n",
    "    passages_block = \"Retrieved passages (id => excerpt):\\n\"\n",
    "    for pid, score, text in passages:\n",
    "        excerpt = text.replace(\"\\n\", \" \").strip()\n",
    "        if len(excerpt) > 800:\n",
    "            excerpt = excerpt[:800] + \"...\"\n",
    "        passages_block += f\"PASSAGE {pid} (score={score:.4f}): {excerpt}\\n\\n\"\n",
    "    instruction = (\"\\nNow provide a concise, grounded interpretation in 2-6 sentences. Use only the passages above. \"\n",
    "                   \"Start the interpretation with 'Interpretation:' and then the content. Return just the interpretation text.\")\n",
    "    return header + json_block + passages_block + instruction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ee82c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7 - normalization and sanity check\n",
    "LENGTH_CHOICES = {\"short\",\"medium\",\"long\"}\n",
    "CLARITY_CHOICES = {\"clear\",\"faint\",\"broken\"}\n",
    "\n",
    "def normalize_json_obj(j: dict) -> dict:\n",
    "    \"\"\"Ensure object matches schema strictly; coerce types and clamp numeric ranges.\"\"\"\n",
    "    out = {\"lines\": {}}\n",
    "    for ln in [\"life_line\",\"head_line\",\"heart_line\",\"fate_line\"]:\n",
    "        v = (j.get(\"lines\") or {}).get(ln)\n",
    "        if not v or not bool(v.get(\"present\")):\n",
    "            out[\"lines\"][ln] = {\"present\": False, \"clarity\": \"\", \"length\": \"\", \"breaks\": 0, \"branches\": 0}\n",
    "            continue\n",
    "        # present True\n",
    "        clarity = str(v.get(\"clarity\",\"\")).strip().lower()\n",
    "        length = str(v.get(\"length\",\"\")).strip().lower()\n",
    "        if clarity not in CLARITY_CHOICES:\n",
    "            clarity = random.choice(list(CLARITY_CHOICES))\n",
    "        if length not in LENGTH_CHOICES:\n",
    "            length = random.choice(list(LENGTH_CHOICES))\n",
    "        try:\n",
    "            breaks = int(v.get(\"breaks\", 0))\n",
    "        except:\n",
    "            breaks = random.randint(*BREAKS_RANGE)\n",
    "        try:\n",
    "            branches = int(v.get(\"branches\", 0))\n",
    "        except:\n",
    "            branches = random.randint(*BRANCHES_RANGE)\n",
    "        # clamp numeric ranges\n",
    "        breaks = max(BREAKS_RANGE[0], min(BREAKS_RANGE[1], breaks))\n",
    "        branches = max(BRANCHES_RANGE[0], min(BRANCHES_RANGE[1], branches))\n",
    "        out[\"lines\"][ln] = {\"present\": True, \"clarity\": clarity, \"length\": length, \"breaks\": breaks, \"branches\": branches}\n",
    "    return out\n",
    "\n",
    "def validate_json_schema(j: dict) -> bool:\n",
    "    try:\n",
    "        assert isinstance(j, dict) and \"lines\" in j\n",
    "        for ln in [\"life_line\",\"head_line\",\"heart_line\",\"fate_line\"]:\n",
    "            v = j[\"lines\"][ln]\n",
    "            assert isinstance(v[\"present\"], bool)\n",
    "            assert isinstance(v[\"clarity\"], str)\n",
    "            assert isinstance(v[\"length\"], str)\n",
    "            assert isinstance(v[\"breaks\"], int)\n",
    "            assert isinstance(v[\"branches\"], int)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ba6a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:   0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama JSON generation error: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:   0%|          | 1/1000 [02:04<34:27:26, 124.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error on interpretation: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:   0%|          | 3/1000 [03:57<20:02:50, 72.39s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error on interpretation: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:   0%|          | 4/1000 [04:59<18:54:21, 68.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error on interpretation: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:   0%|          | 5/1000 [06:01<18:16:30, 66.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error on interpretation: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:   1%|          | 6/1000 [07:03<17:52:59, 64.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error on interpretation: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:   1%|          | 9/1000 [09:56<16:37:55, 60.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error on interpretation: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:   1%|          | 10/1000 [10:58<16:45:40, 60.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error on interpretation: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n",
      "Ollama JSON generation error: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:   1%|          | 11/1000 [13:03<22:03:34, 80.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error on interpretation: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:   1%|▏         | 13/1000 [15:05<19:20:28, 70.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error on interpretation: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:   1%|▏         | 14/1000 [16:07<18:37:35, 68.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error on interpretation: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:   2%|▏         | 15/1000 [17:09<18:07:29, 66.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error on interpretation: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:   2%|▏         | 18/1000 [20:09<17:00:32, 62.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error on interpretation: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:   2%|▏         | 19/1000 [21:11<16:58:26, 62.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error on interpretation: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:   2%|▏         | 20/1000 [22:13<16:56:39, 62.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error on interpretation: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n",
      "Ollama JSON generation error: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:   2%|▏         | 21/1000 [24:17<21:59:00, 80.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error on interpretation: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:   2%|▏         | 22/1000 [25:19<20:26:21, 75.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error on interpretation: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:   2%|▏         | 23/1000 [26:21<19:20:59, 71.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error on interpretation: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:   2%|▏         | 24/1000 [27:23<18:35:13, 68.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error on interpretation: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:   2%|▎         | 25/1000 [28:26<18:02:52, 66.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error on interpretation: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:   3%|▎         | 26/1000 [29:28<17:39:52, 65.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error on interpretation: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:   3%|▎         | 27/1000 [30:30<17:23:24, 64.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error on interpretation: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:   3%|▎         | 28/1000 [31:32<17:11:37, 63.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error on interpretation: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:   3%|▎         | 29/1000 [32:34<17:03:10, 63.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error on interpretation: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:   3%|▎         | 30/1000 [33:36<16:57:00, 62.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error on interpretation: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n",
      "Ollama JSON generation error: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:   3%|▎         | 31/1000 [35:41<21:53:01, 81.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error on interpretation: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:   3%|▎         | 32/1000 [36:43<20:18:58, 75.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error on interpretation: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:   3%|▎         | 33/1000 [37:45<19:12:50, 71.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error on interpretation: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:   4%|▎         | 35/1000 [39:49<17:51:08, 66.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error on interpretation: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:   4%|▎         | 36/1000 [40:51<17:28:33, 65.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error on interpretation: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:   4%|▎         | 37/1000 [41:53<17:12:25, 64.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error on interpretation: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:   4%|▍         | 38/1000 [42:55<17:00:49, 63.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error on interpretation: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:   4%|▍         | 39/1000 [43:57<16:52:26, 63.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error on interpretation: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:   4%|▍         | 40/1000 [44:59<16:46:11, 62.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error on interpretation: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n",
      "Ollama JSON generation error: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:   4%|▍         | 41/1000 [47:03<21:39:07, 81.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error on interpretation: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:   4%|▍         | 43/1000 [49:02<18:43:18, 70.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error on interpretation: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:   4%|▍         | 44/1000 [50:05<18:02:28, 67.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error on interpretation: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:   4%|▍         | 45/1000 [51:07<17:33:40, 66.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error on interpretation: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:   5%|▍         | 46/1000 [52:09<17:13:18, 64.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error on interpretation: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:   5%|▍         | 47/1000 [53:11<16:58:35, 64.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error on interpretation: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:   5%|▌         | 50/1000 [56:16<16:30:47, 62.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error on interpretation: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n",
      "Ollama JSON generation error: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:   5%|▌         | 51/1000 [58:20<21:22:00, 81.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error on interpretation: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:   5%|▌         | 52/1000 [59:22<19:51:01, 75.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error on interpretation: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:   5%|▌         | 54/1000 [1:01:04<16:46:56, 63.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error on interpretation: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:   6%|▌         | 57/1000 [1:03:29<14:28:04, 55.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error on interpretation: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:   6%|▌         | 60/1000 [1:05:48<12:46:36, 48.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama JSON generation error: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:   6%|▋         | 63/1000 [1:09:14<15:13:20, 58.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error on interpretation: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:   6%|▋         | 64/1000 [1:10:16<15:29:27, 59.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error on interpretation: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:   6%|▋         | 65/1000 [1:11:18<15:40:26, 60.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error on interpretation: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:   7%|▋         | 68/1000 [1:13:57<14:42:33, 56.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error on interpretation: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:   7%|▋         | 70/1000 [1:15:38<13:49:17, 53.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama JSON generation error: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:   7%|▋         | 71/1000 [1:17:42<19:16:42, 74.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error on interpretation: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:   7%|▋         | 72/1000 [1:18:44<18:17:24, 70.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error on interpretation: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:   7%|▋         | 73/1000 [1:19:46<17:35:26, 68.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error on interpretation: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:   7%|▋         | 74/1000 [1:20:49<17:05:47, 66.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error on interpretation: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:   8%|▊         | 75/1000 [1:21:51<16:44:50, 65.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error on interpretation: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:   8%|▊         | 76/1000 [1:22:53<16:29:41, 64.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error on interpretation: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:   8%|▊         | 78/1000 [1:24:56<16:06:50, 62.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error on interpretation: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:   8%|▊         | 79/1000 [1:25:58<16:02:13, 62.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error on interpretation: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:   8%|▊         | 80/1000 [1:26:55<15:32:46, 60.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama JSON generation error: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:   8%|▊         | 81/1000 [1:28:59<20:22:58, 79.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error on interpretation: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:   9%|▉         | 88/1000 [1:34:24<13:38:16, 53.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error on interpretation: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:   9%|▉         | 90/1000 [1:35:54<12:25:31, 49.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama JSON generation error: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:  10%|▉         | 95/1000 [1:41:08<14:19:54, 57.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error on interpretation: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:  10%|█         | 100/1000 [1:45:34<13:49:21, 55.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error on interpretation: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n",
      "Ollama JSON generation error: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:  10%|█         | 101/1000 [1:47:38<18:58:22, 75.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error on interpretation: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:  10%|█         | 103/1000 [1:49:35<16:45:35, 67.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error on interpretation: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:  11%|█         | 109/1000 [1:54:34<13:13:04, 53.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error on interpretation: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:  11%|█         | 110/1000 [1:55:18<12:29:37, 50.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama JSON generation error: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:  11%|█▏        | 114/1000 [1:59:54<14:59:01, 60.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error on interpretation: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:  12%|█▏        | 119/1000 [2:04:05<12:59:23, 53.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error on interpretation: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs:  12%|█▏        | 120/1000 [2:05:08<13:38:30, 55.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error on interpretation: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    }
   ],
   "source": [
    "# Cell 8 - main generation loop\n",
    "def produce_jsonl_pairs(target_n: int = NUM_PAIRS, out_file: str = OUTPUT_JSONL,\n",
    "                        batch_size: int = BATCH_JSONS_PER_CALL, top_k: int = TOP_K):\n",
    "    produced = 0\n",
    "    seen_inputs = set()\n",
    "    Path(out_file).parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(out_file, \"w\", encoding=\"utf-8\") as fout:\n",
    "        pbar = tqdm(total=target_n, desc=\"Generating pairs\")\n",
    "        while produced < target_n:\n",
    "            # 1) request a batch of random JSONs from Ollama\n",
    "            try:\n",
    "                prompt = prompt_generate_jsons(batch_n=batch_size)\n",
    "                raw = call_ollama(prompt, model=OLLAMA_MODEL, max_tokens=2048, temperature=1.0)\n",
    "                objs = extract_json_list_from_text(raw)\n",
    "            except Exception as e:\n",
    "                print(\"Ollama JSON generation error:\", e)\n",
    "                # fallback to simple python sampling if Ollama fails\n",
    "                objs = []\n",
    "                for _ in range(batch_size):\n",
    "                    objs.append({\n",
    "                        \"lines\":{\n",
    "                            ln: ({\"present\": False, \"clarity\": \"\", \"length\": \"\", \"breaks\":0, \"branches\":0} \n",
    "                                 if random.random() < 0.15 else\n",
    "                                 {\"present\": True,\n",
    "                                  \"clarity\": random.choice(list(CLARITY_CHOICES)),\n",
    "                                  \"length\": random.choice(list(LENGTH_CHOICES)),\n",
    "                                  \"breaks\": random.randint(*BREAKS_RANGE),\n",
    "                                  \"branches\": random.randint(*BRANCHES_RANGE)})\n",
    "                            for ln in [\"life_line\",\"head_line\",\"heart_line\",\"fate_line\"]\n",
    "                        }\n",
    "                    })\n",
    "            # 2) normalize & dedup\n",
    "            normalized_batch = []\n",
    "            for obj in objs:\n",
    "                try:\n",
    "                    norm = normalize_json_obj(obj)\n",
    "                    key = json.dumps(norm, sort_keys=True)\n",
    "                    if key in seen_inputs:\n",
    "                        continue\n",
    "                    seen_inputs.add(key)\n",
    "                    normalized_batch.append(norm)\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "\n",
    "            # 3) for each normalized json do RAG retrieve and ask Ollama to interpret\n",
    "            for j_in in normalized_batch:\n",
    "                if produced >= target_n:\n",
    "                    break\n",
    "                # retrieve passages\n",
    "                passages = retrieve_passages_for_json(j_in, top_k=top_k)\n",
    "                # build prompt and generate interpretation (low temperature)\n",
    "                rag_prompt = prompt_rag_interpretation(j_in, passages)\n",
    "                try:\n",
    "                    interp_raw = call_ollama(rag_prompt, model=OLLAMA_MODEL, max_tokens=1024, temperature=RAG_TEMPERATURE)\n",
    "                except Exception as e:\n",
    "                    print(\"Ollama error on interpretation:\", e)\n",
    "                    interp_raw = \"\"\n",
    "                # best effort: try to extract the interpretation text (we asked to start with 'Interpretation:')\n",
    "                interp = interp_raw.strip()\n",
    "                # If the model returned JSON or extra, try to isolate 'Interpretation:' line\n",
    "                m = re.search(r'Interpretation:\\s*(.+)', interp, re.S)\n",
    "                if m:\n",
    "                    interp = m.group(1).strip()\n",
    "                # fallback: if empty, set a placeholder\n",
    "                if not interp:\n",
    "                    interp = \"(No interpretation generated - fallback) \"\n",
    "\n",
    "                # save entry\n",
    "                entry = {\n",
    "                    \"input\": j_in,\n",
    "                    \"output\": interp,\n",
    "                    \"retrieved\": [{\"id\": pid, \"score\": score, \"excerpt\": txt[:800]} for pid, score, txt in passages]\n",
    "                }\n",
    "                fout.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "                produced += 1\n",
    "                pbar.update(1)\n",
    "                time.sleep(OLLAMA_SLEEP)  # small delay to be polite\n",
    "        pbar.close()\n",
    "    print(f\"Finished. Wrote {produced} pairs to {out_file}\")\n",
    "\n",
    "# run\n",
    "produce_jsonl_pairs(target_n=NUM_PAIRS, out_file=OUTPUT_JSONL, batch_size=BATCH_JSONS_PER_CALL, top_k=TOP_K)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6ba4f37",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OUTPUT_JSONL' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# sample a few\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m sample_jsonl(\u001b[43mOUTPUT_JSONL\u001b[49m, k=\u001b[32m3\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'OUTPUT_JSONL' is not defined"
     ]
    }
   ],
   "source": [
    "# Cell 9 - inspect a few random examples\n",
    "def sample_jsonl(path: str, k: int = 5):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    print(f\"Total entries: {len(lines)}\")\n",
    "    import random\n",
    "    for item in random.sample(lines, min(k, len(lines))):\n",
    "        j = json.loads(item)\n",
    "        print(\"INPUT:\")\n",
    "        print(json.dumps(j[\"input\"], indent=2, ensure_ascii=False))\n",
    "        print(\"RETRIEVED (top2):\")\n",
    "        for r in j.get(\"retrieved\", [])[:2]:\n",
    "            print(\"  id:\", r[\"id\"], \"score:\", r[\"score\"])\n",
    "            print(\"  excerpt:\", r[\"excerpt\"][:300].replace(\"\\n\",\" \"), \"...\")\n",
    "        print(\"OUTPUT:\")\n",
    "        print(j[\"output\"])\n",
    "        print(\"-\"*60)\n",
    "\n",
    "# sample a few\n",
    "sample_jsonl(OUTPUT_JSONL, k=3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "palmistry_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
